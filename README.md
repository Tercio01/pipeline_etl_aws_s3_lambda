# pipeline_etl_aws_s3_lambda
Pipeline de ETL serveless na AWS que processa dados JSON de pedidos, transforma com Python/Pandas e armazena em formato Parquet.
# Pipeline de ETL Serverless na AWS para Processamento de Pedidos

## üìñ Sobre o Projeto

Este projeto demonstra a constru√ß√£o de um pipeline de dados 100% serverless na nuvem AWS. O objetivo √© simular um cen√°rio real de e-commerce, onde dados de pedidos s√£o gerados em formato JSON, processados em tempo real e armazenados em um formato anal√≠tico otimizado (Parquet) em um Data Lake.

Esta √© uma solu√ß√£o robusta, escal√°vel e de baixo custo, utilizando as melhores pr√°ticas de arquitetura orientada a eventos.

---

## üèõÔ∏è Arquitetura da Solu√ß√£o

O fluxo de dados segue a seguinte arquitetura:

1. **Ingest√£o:** Arquivos JSON contendo dados de pedidos s√£o enviados para um bucket S3, em uma pasta de dados brutos (`incoming`).
2. **Gatilho (Trigger):** O evento de cria√ß√£o de um novo objeto no S3 aciona automaticamente uma fun√ß√£o AWS Lambda.
3. **Processamento (ETL):** A fun√ß√£o Lambda, escrita em Python, executa as seguintes tarefas:
    * L√™ o arquivo JSON do S3.
    * Utiliza a biblioteca **Pandas/AWS Data Wrangler** para achatar a estrutura aninhada do JSON e limpar os dados.
    * Converte o DataFrame processado para o formato **Apache Parquet**.
4. **Armazenamento:** O arquivo Parquet resultante √© salvo em outra pasta no mesmo bucket S3, servindo como nosso Data Lake.

![Diagrama da Arquitetura](https://github.com/user-attachments/assets/34d6c5d2-5361-4379-9c5f-a5cb780df9a8)

---

## üõ†Ô∏è Tecnologias Utilizadas

As seguintes ferramentas e tecnologias foram utilizadas na constru√ß√£o do projeto:

* **Cloud Provider:** AWS (Amazon Web Services)
* **Armazenamento:** Amazon S3
* **Processamento Serverless:** AWS Lambda
* **Seguran√ßa e Permiss√µes:** AWS IAM (Identity and Access Management)
* **Monitoramento e Logs:** AWS CloudWatch
* **Linguagem de Programa√ß√£o:** Python 3.9
* **Bibliotecas Principais:** Pandas, AWS Data Wrangler (para intera√ß√£o com servi√ßos AWS)
* **Formato de Dados:** JSON (origem), Apache Parquet (destino)
* **Infraestrutura como C√≥digo (IaC):** Configura√ß√£o realizada via Console da AWS.

---

## Ï±å Desafios e Aprendizados

A constru√ß√£o deste projeto foi uma jornada de grande aprendizado. Um dos principais desafios foi a configura√ß√£o correta das permiss√µes no **IAM**, garantindo que a fun√ß√£o Lambda tivesse acesso para ler do S3 e escrever logs no CloudWatch, seguindo o princ√≠pio do menor privil√©gio. Outro ponto crucial foi o gerenciamento de depend√™ncias externas (Pandas e AWS Wrangler) atrav√©s das **AWS Lambda Layers**, um conceito fundamental para o desenvolvimento serverless. A depura√ß√£o de erros utilizando os logs do CloudWatch foi essencial para o sucesso do pipeline.

---

## üöÄ Como Executar o Projeto

Para replicar este projeto, siga os seguintes passos:
1. Crie um bucket no S3 com duas pastas: `orders_json_incoming/` e `orders_parquet_datalake/`.
2. Crie uma fun√ß√£o Lambda com o c√≥digo dispon√≠vel em `/src/lambda_function.py`.
3. Configure as permiss√µes no IAM e as Lambda Layers necess√°rias.
4. Configure o gatilho do S3 na fun√ß√£o Lambda para monitorar a pasta de entrada.
5. Fa√ßa o upload do arquivo `orders_etl.json` para a pasta `orders_json_incoming/` para iniciar o pipeline.
